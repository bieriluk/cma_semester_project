---
title: "Project code"
author: "Lukas Bieri (bieriluk) & Valentin Hett (hettval1)"
date: "17.05.2023"
format: html
editor: source
---



# Improved Transport Mode Detection & Public Transport Punctuality Assessment

## 1. Informationen & Metadaten

### 1.1 Zum Projekt

Fallbeispiel mit Abschlussbericht über die multivariate Analyse am Beispiel der Habitatselektion des Rehs

**Modul:** Research Methods HS22

**Kurs:** Fallstudie MRU Biodiversity and Ecosystems

**Dozierende:** Roland Graf (graf) & Reto Rupf (rupf)

**Unterstützende Lehrpersonen:** Benjamin Sigrist (sigb) & Adrian Hochreutener (hoce)

**Autoren:** Mirjam Scheib (scheimir), Valentin Hett (hettval1), Pascal Luder (luderpas), Lukas Bieri (bieriluk)

**Stand:** 05.01.2023



### 1.2 Ordnerstruktur R-Projekt

Arbeiten erfolgten in einem R-Projekt auf der MS OneDrive über den Account der ZHAW, geteilt mit allen am Projekt mitarbeitenden Personen.

Das Projekt wurde in folgendem Ordner erstellt:
Shared_RProject_ReMe_Team3

Daten welche im R versendet wurden, sind im Unterordner "data" abgelegt.
Resultate, inbesondere Grafiken, wurden im Unterordner "Results" abgelegt.

### 1.3 Verwendete Daten

Verwendete Daten wurden von den Dozierenden und unterstützenden Lehrpersonen beschafft und zur Verfügung gestellt. Für die Auswertung wurden keine eigenen Daten beschafft oder erhoben.

Folgende Daten wurden für die Auswertungen und Modellierungen in der vorliegenden Fallstudie verwendet:

1.  File: Aufgabe3_Homeranges_Rehe_landforst_20221024.csv
    Kurzbeschrieb:
    Stand:
    Metadaten:

2.  File: Aufgabe3_Reh_Waldstruktur_221013.csv
    Kurzbeschrieb:
    Stand:
    Metadaten:

3.  File: Aufgabe4_Datensatz_Habitatnutzung_Modelle_20221031_moodle.csv
    Kurzbeschrieb:
    Stand:
    Metadaten:

4.  File: pk25_wpz.tif
    Kurzbeschrieb:
    Stand:
    Metadaten:

Alle heruntergeladen am 15.11.2022 vom Moodle-Server der ZHAW LSFM im Kurs Research Methods HS22 MSc ENR, Ordner ReMe HS22 MSc ENR/Fallstudie Biodiversity & Ecosystems/N_Daten: https://moodle.zhaw.ch/mod/folder/view.php?id=578940

Ein Grossteil des Codes in R wurde aus den von der Modulleitung auf GitHub zu Verfügung gestellten Übungen zur Fallstudie übernommen und für unsere Zwecke angepasst und ergänzt.

### 1.4 Verwendete Software

**R version 4.2.1 (2022-06-23 ucrt)** -- "Funny-Looking Kid" Copyright (C) 2022 The R Foundation for Statistical Computing Platform: x86_64-w64-mingw32/x64 (64-bit)

**RStudio 2022.07.2+576** "Spotted Wakerobin" Release (e7373ef832b49b2a9b88162cfe7eac5f22c40b34, 2022-09-06) for Windows Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) QtWebEngine/5.12.8 Chrome/69.0.3497.128 Safari/537.36


## 2. Vorbemerkungen & Vorbereitung

Zur Vorbereitung: R-Session neu starten (Restart R) und Konsole berenigen (clear console)

Dann: Funktion die wenn nötig die nötigen Packages installiert und lädt : - lme4 - bbmle - MuMIn - tidyverse - DHARMa - car - MASS - ROCR - sjPlot - ggeffects - sjstats - cowplot - magrittr - gstat - sf - raster - adehabitatHR - maptools - sp - ggspatial - rgeos - rgdal - PerformanceAnalytics - pastecs

```{r}
#| output: false
#| warning: false
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, repos = "http://cran.us.r-project.org", 
                     dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

packages <- c("ggplot2", "dplyr", "tidyr", "readr", "zoo", "data.table", "sf", "terra", "tmap", "stats", "randomForest", "lubridate", "trajr", "gstat")

ipak(packages)
```
Prüfen ob die benötigten Packages korrekt geladen wurden.
i.O. --> Weiter zum einlesen der Daten

## 3. Daten(vor)verarbeitung

### 3.1 Daten einlesen & anpassen

**Daten für die Homerange-Berechnung einlesen**
```{r}
posmo <- read_delim("datasets/posmo_2023-01-01T00-00-00_2023-05-13T23-59-59_bieriluk.csv", delim = ",")

```

Check if the import got the Time Zone for the POSIXct colum correct (compare with the first entry in the imported data with the raw data):
```{r}
str(posmo)
Sys.time()
```

Store your data frame as a spatial data frame and transform the coordinate system from WGS84 (i.e. EPSG 4326) to CH1903+ LV95 (EPSG 2056)
```{r}
posmo <- st_as_sf(posmo, coords = c("lon_x","lat_y"), crs = 4326) |>
  st_transform(2056)

head(posmo)
```

Extract the coordinates into separate colums to use them for euclidean distance calculation:
```{r}
posmo_coordinates <- st_coordinates(posmo)

posmo <- cbind(posmo, posmo_coordinates)
```


### 3.2 For how long were the individual tracked? Are there gaps? Were all individuals tracked concurrently or sequentially?
```{r}

head(posmo)
tail(posmo)
```




## 3. Getting an overview & EDA

### 3.1 How many individuals were tracked 
```{r}
posmo$user_id |> unique()
```
--> 1 Individuals, because for the moment we are only working with on data set (not the POSMO Pool)

Choose a single day for this step (13.04.2023, because many different visited places that day) and filter your data:
```{r}
posmo_filter <- posmo |>
    filter(as.Date(datetime) == "2023-04-13")
```

```{r}
ggplot(posmo_filter, aes(X,Y, color = datetime)) +
  geom_point() +
  geom_path() +
  coord_equal()

tmap_mode(mode = "view")

tm_shape(posmo_filter) +
  tm_dots()
  tm_basemap("Esri.WorldImagery")
```
X = E, Y = N


### Remove deuplicate time stamps
Calculate timelag and also remove values with the same time stamp by summarising unsing the mean.
```{r}

posmo_filter_clean <- posmo_filter|>
  st_drop_geometry()|>
  select(datetime, X, Y)|>
  mutate(
    datetime = unclass(datetime)
    ) |>
  group_by(datetime)|>
  summarise(
    X = mean(X, na.rm = TRUE),
    Y = mean(Y, na.rm = TRUE)
  ) |>
  ungroup()|>
  mutate(
    datetime = as.POSIXct(datetime, origin = '1970-01-01', tz = "UTC")
    )|>
  st_as_sf(coords = c("X","Y"), crs = 2056, remove = FALSE) 

n_distinct(posmo_filter_clean$datetime)
n_distinct(posmo_filter$datetime)
```


### 3.3 What is the temporal sampling interval between the locations?
Calculate timelag to figure out a appropriate temporal window:
```{r}
posmo_filter_clean <- posmo_filter_clean |>
  mutate(timelag_s = as.numeric(difftime(lead(datetime), datetime)))

```

Look for negative timelag values and visually validate that they are corrective algorith errors, then eliminate these rows incl. NA's. 
```{r}
posmo_filter_clean |>
    filter(timelag_s < 0)

posmo_filter_clean |>
    filter(is.na(timelag_s))

tm_shape(slice(posmo_filter_clean,1261)) +
  tm_dots()
  tm_basemap("Esri.WorldImagery")
  
posmo_filter_clean <- posmo_filter_clean |>
    filter(timelag_s > 0)

```


So what does the timelag between measurement points look like:
```{r}
tail(posmo_filter_clean)
mean(posmo_filter_clean$timelag_s, na.rm = TRUE)
median(posmo_filter_clean$timelag_s, na.rm = TRUE)
min(posmo_filter_clean$timelag_s, na.rm = TRUE)
max(posmo_filter_clean$timelag_s, na.rm = TRUE)


posmo_filter_clean|> 
  ggplot(aes(timelag_s)) +
  geom_histogram(binwidth = 1) +
  lims(x = c(0, 20000)) +
  scale_y_log10() +
  scale_x_log10()

posmo_filter_clean |> 
  ggplot(aes(datetime, timelag_s)) +
  geom_point() + 
  geom_line()

```


Calculate further features for outlier detection
```{r}
posmo_filter_clean <- posmo_filter_clean |> 
  mutate(steplength_m = sqrt((X-lead(X,1))^2 + (Y-lead(Y,1))^2)) |> 
  mutate(speed_ms = steplength_m/timelag_s)
posmo_filter_clean
```
X = E, Y = N

Identify and remove outliers
```{r}
mean(posmo_filter_clean$speed_ms, na.rm = TRUE)
median(posmo_filter_clean$speed_ms, na.rm = TRUE)
min(posmo_filter_clean$speed_ms, na.rm = TRUE)
max(posmo_filter_clean$speed_ms, na.rm = TRUE)

posmo_filter_clean |>
    filter(speed_ms > 140)

tm_shape(slice(posmo_filter_clean,1258:1260)) +
  tm_dots()
  tm_basemap("Esri.WorldImagery")
```


Resampling

Adding new rows with the resampled time stamps and linearly interpolating them using na.approx() 
```{r}
# Create a new df with all necessary time stamps

resamp_timestamps <- seq.POSIXt(from = ceiling_date(min(posmo_filter_clean$datetime), "10 sec"),
                                to = floor_date(max(posmo_filter_clean$datetime), "10 sec"), 
                                by = 10)
resamp_timestamps <- as.data.frame(resamp_timestamps)

resamp_timestamps <- resamp_timestamps |> 
  as.data.frame() |> 
  rename(datetime = resamp_timestamps)

# Combine the df with the resulting df adding rows with NA for the nessecary time stamps
posmo_filter_resamp <- posmo_filter_clean |>
  select(datetime, X, Y, geometry) |>
  full_join(resamp_timestamps, by = "datetime") |>
  arrange(datetime)

n_distinct(posmo_filter_resamp$datetime)
sum(n_distinct(posmo_filter_clean$datetime), n_distinct(resamp_timestamps$datetime))

# Linearly interpolate the missing values (coordinates) & Filter to only the resampled rows
posmo_filter_approx <- posmo_filter_resamp |>
  st_drop_geometry()|>
  mutate(X = na.approx(X),
         Y = na.approx(Y)
         ) |>
  filter(second(datetime) %in%  c(0, 10, 20, 30, 40, 50)) |>
  st_as_sf(coords = c("X","Y"), crs = 2056, remove = FALSE)

posmo_filter_approx

tm_shape(posmo_filter_approx) +
  tm_dots()
  tm_basemap("Esri.WorldImagery")

```

### Segmentation

#### 3.2.1 Get an overview
```{r}
ggplot(posmo_filter_approx, aes(X,Y, color = datetime)) +
  geom_point() +
  geom_path() +
  coord_equal()
```
X = E, Y = N

```{r}
posmo_filter_approx|> 
  head(50) |> 
  ggplot(aes(datetime, 1)) +
  geom_point()
```

#### 3.2.2 (a) Specify a temporal windows v for in which to measure Euclidean distances
Calculate timelag to figure out a appropriate temporal window:
```{r}
posmo_filter_approx <- posmo_filter_approx|>
  mutate(timelag_s = as.numeric(difftime(lead(datetime), datetime)))
```

So what does the timelag between measurement points look like:
```{r}
tail(posmo_filter_approx)
mean(posmo_filter_approx$timelag_s, na.rm = TRUE)
median(posmo_filter_approx$timelag_s, na.rm = TRUE)
min(posmo_filter_approx$timelag_s, na.rm = TRUE)
max(posmo_filter_approx$timelag_s, na.rm = TRUE)

```

For this exercise we try to use the data as is and try it with a window of 6 steps and 6 steps forward and backward in the data. This is v = 2min, because the timelag is 10s.

#### 3.2.3 (b) Measure the distance from every point to every other point within this temporal window v
```{r}
posmo_filter_approx <- posmo_filter_approx |> 
  mutate(
    n_plus10 = sqrt((lead(X,10) - X)^2 + (lead(Y,10) - Y)^2),
    n_plus20 = sqrt((lead(X,20) - X)^2 + (lead(Y,20) - Y)^2),
    n_minus10 = sqrt((lag(X,10) - X)^2 + (lag(Y,10) - Y)^2),
    n_minus20 = sqrt((lag(X,20) - X)^2 + (lag(Y,20) - Y)^2)
  )
```
Segmentation: ave.distance d =3m, temporal window v = 300s, Filtering: 60s


```{r}
posmo_filter_approx <- posmo_filter_approx |> 
  rowwise() |> 
  mutate(
    stepMean = mean(c(n_minus10, n_minus20, n_plus10, n_plus20))
  ) |> 
  ungroup()
```
Careful, you need rowwise() for the mutate() function to caclulate mean() for every row. You need to ungroup it in the end, because your data frame has the rowwise grouping saved.

#### 3.2.4 (c) Remove “static points”: These are points where the average distance is less than a given threshold

We need to determine the appropriate threshold where the points are stationary. It helps to visualize the data:
```{r}
ggplot(posmo_filter_approx, aes(stepMean)) +
  geom_histogram(binwidth = 10) +
  geom_vline(xintercept = median(posmo_filter$stepMean, na.rm = TRUE))
```


### 3.3 Task 2: Specify and apply threshold d
To find a good threshold, I tried different distances which I would realistically travel when not being stationary.
```{r}
posmo_filter_segm <- posmo_filter_approx |>
    ungroup() |>
    mutate(static = stepMean < 120) |>
    drop_na(static)
```

### 3.4 Task 3: Visualize segmented trajectories
```{r}
ggplot(posmo_filter_segm, aes(X,Y)) +
  geom_path() +
  geom_point(aes(color = static)) +
  coord_equal()

tm_shape(posmo_filter_segm) +
  tm_dots(col = "static") +
  tm_basemap("Esri.WorldImagery")

```
Moving 10 in approx. 300sec being the threshold for being stationary seems to line up well with the reality when visualizing it.

### 3.5 Task 4: Segment-based analysis
Create function for segmentation at the stationary points:
```{r}
rle_id <- function(vec) {
    x <- rle(vec)$lengths
    as.factor(rep(seq_along(x), times = x))
}
```

Run the function on the data:
```{r}
posmo_filter_segm <- posmo_filter_segm |>
    mutate(segment_id = rle_id(static))

head(posmo_filter_segm)
```

Visualize the segments:
```{r}
ggplot(posmo_filter_segm, aes(X,Y)) +
  geom_path() +
  geom_point(aes(color = segment_id)) +
  coord_equal()

tm_shape(posmo_filter_segm) +
  tm_dots(col = "segment_id") +
  tm_basemap("Esri.WorldImagery")
```

Seems to line up well with ground truth for that day and the trips I took. If I would want to improve the segmentation, I would have to compare the details with reality and maybe smoothen out the differences in timelags.

Unsupervised algorithm using k-means clustering


GIS multi-criteria process 


Supervised learning algorithms


